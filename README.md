# ROBOTS_CHECKER

A web-based tool that allows users to upload a CSV file (maximum 500 URLs) to check if the URLs are blocked by a `robots.txt` file. Built with Node.js and Express.js, it utilizes Multer for file uploads and CSV Parser for data extraction. The results are downloadable as a CSV file.

## Features

- Upload a CSV file containing up to 500 URLs.
- Checks each URL against the respective site's `robots.txt` file to determine if it's blocked.
- Provides the results in a downloadable CSV format.
- Efficient file handling and processing.
- Deployed on Render for easy access.

## Demo

Access the live application here: [robots-checker.onrender.com](https://robots-checker.onrender.com)

## Technologies Used

![Node.js](https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=node.js&logoColor=white)
![Express.js](https://img.shields.io/badge/Express.js-000000?style=for-the-badge&logo=express&logoColor=white)
![Multer](https://img.shields.io/badge/Multer-7B9F35?style=for-the-badge&logo=npm&logoColor=white)
![CSV Parser](https://img.shields.io/badge/CSV%20Parser-005A71?style=for-the-badge&logo=npm&logoColor=white)

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/ArpitVadariya/ROBOTS_CHECKER.git
   ```
